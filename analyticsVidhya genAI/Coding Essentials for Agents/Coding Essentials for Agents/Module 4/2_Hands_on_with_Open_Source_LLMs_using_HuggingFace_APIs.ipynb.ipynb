{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUwrTGNs4kSN"
      },
      "source": [
        "<center> <h1> Prompt Engineering with Open-Source Large Language Models (LLMs) using HuggingFace InferenceClient</h1> </center>\n",
        "\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "\n",
        "_____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGwBgKICV_DJ"
      },
      "source": [
        "\n",
        "In this notebook we will learn how to run any open-source LLMs via HuggingFace InferenceClient using this colab notebook. You can run this notebook in your local server also without worrying about having enough infrastructure to run these models!\n",
        "\n",
        "The HuggingFace [__InferenceClient__](https://huggingface.co/docs/huggingface_hub/guides/inference) provides easy access to thousands of models without worrying about your infrastructure.\n",
        "\n",
        "The models we will be trying here include:\n",
        "\n",
        "- __[meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)__ - Meta's powerful 8B parameter instruction-tuned model from the Llama 3.1 family, designed for conversational AI and instruction-following tasks.\n",
        "\n",
        "- __[deepseek-ai/DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)__ - DeepSeek's advanced V3 model optimized for various language understanding and generation tasks.\n",
        "\n",
        "\n",
        "__You just need an internet connection and a HuggingFace Account and API Key to use these models.__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6K23EM5EMol"
      },
      "outputs": [],
      "source": [
        "# Install required package\n",
        "!pip install huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uHHQuoUbEMom"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Vipin Vashisth\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w81pr0vzhNaJ"
      },
      "source": [
        "## Get your API Key\n",
        "\n",
        "Remember to go to your [HuggingFace Account Settings](https://huggingface.co/settings/account) and generate an API key by creating a new token from the [Access Tokens](https://huggingface.co/settings/tokens) section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGSuxLmAiCGP"
      },
      "source": [
        "## Load HuggingFace API Credentials\n",
        "\n",
        "Enter your key from [here](https://huggingface.co/settings/tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_YjkI6sVG7H",
        "outputId": "2ed2cb8e-20af-4d53-b13a-67e1b4e3d3f5"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "API_KEY = getpass(\"Enter HuggingFace API Key: \")\n",
        "os.environ['HF_TOKEN'] = API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgZzsDCZiN3k"
      },
      "source": [
        "### Initialize InferenceClient\n",
        "\n",
        "Here we create InferenceClient instances for our models. The InferenceClient provides a simple interface to access HuggingFace models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-TRno7hBB_qX"
      },
      "outputs": [],
      "source": [
        "# Initialize client with API key for models requiring authentication\n",
        "client_with_auth = InferenceClient(api_key=os.environ[\"HF_TOKEN\"])\n",
        "\n",
        "# Initialize client without API key for publicly available models\n",
        "client = InferenceClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BPGezswkYNZ"
      },
      "source": [
        "## Define Query Functions\n",
        "\n",
        "Here we create helper functions to query our models easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WtpZr9DlXHPZ"
      },
      "outputs": [],
      "source": [
        "def query_llama(prompt, system_message=None):\n",
        "    \"\"\"\n",
        "    Query Llama-3.1-8B-Instruct model\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    completion = client_with_auth.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=messages,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "def query_deepseek(prompt, system_message=None):\n",
        "    \"\"\"\n",
        "    Query DeepSeek-V3-0324 model\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
        "        messages=messages,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "repJ_4TCllJO"
      },
      "source": [
        "## Prompting with Open-Source LLM APIs\n",
        "\n",
        "Now we will use HuggingFace InferenceClient and try some tasks with prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjD6KmrBluQE"
      },
      "source": [
        "### 1. Basic Q & A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JIyiS1TtYfMi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Can you explain what is quantum computing to a 5th grader?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Can you explain what is quantum computing to a 5th grader?\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w9y8uhKXOB7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Imagine you have a big box of different colored socks. You want to find a specific pair of socks, but they're all mixed up inside the box. A regular computer would look at each sock one by one, and it would take a long time to find the pair you're looking for.\n",
              "\n",
              "A quantum computer is like a super-smart, magic box that can look at all the socks at the same time. It can even look at all the possible combinations of socks, like \"sock 1 with sock 2\" or \"sock 3 with sock 4.\" This means it can find the pair of socks you're looking for much, much faster than a regular computer.\n",
              "\n",
              "But how does it do that? Well, quantum computers use something called \"qubits\" (say \"cue-bits\"). Qubits are like special kinds of socks that can be in many different states at the same time. It's like a sock that's both red and blue at the same time!\n",
              "\n",
              "This means that a quantum computer can process many different possibilities all at once, which makes it really good at solving certain kinds of problems. It's like having a super-powerful, magic calculator that can help us solve really hard puzzles.\n",
              "\n",
              "So, quantum computing is a way of using special computers to solve problems that are too hard for regular computers. It's like having a magic tool that can help us find the answers to really tricky questions!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query Llama model\n",
        "print(\"=\" * 50)\n",
        "print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
        "print(\"=\" * 50)\n",
        "response_llama = query_llama(prompt)\n",
        "display(Markdown(response_llama))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y3cJytUzYc4I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DEEPSEEK-V3 RESPONSE:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Sure! Here's a simple way to explain **quantum computing** to a 5th grader:\n",
              "\n",
              "---\n",
              "\n",
              "### **Imagine a Super-Powerful Computer!**\n",
              "A **quantum computer** is like a super-smart computer that uses tiny, tiny things called **qubits** (like the \"bits\" in regular computers, but way cooler!). \n",
              "\n",
              "### **Regular Computers vs. Quantum Computers**\n",
              "- **Regular computers** (like your tablet or laptop) use **bits** that can be either **0 or 1** (like a light switchâ€”ON or OFF).  \n",
              "- **Quantum computers** use **qubits**, which can be **0, 1, or BOTH at the same time** (like a spinning coin thatâ€™s both heads AND tails until you catch it!).  \n",
              "\n",
              "### **Why Is That Cool?**\n",
              "Because qubits can be in many states at once, quantum computers can solve **really hard problems** super fastâ€”like:  \n",
              "- Figuring out the best way to **cure diseases** by testing millions of medicine combinations.  \n",
              "- Helping scientists invent **new materials** (like unbreakable stuff for spaceships!).  \n",
              "- Making **super-secret codes** that hackers canâ€™t break.  \n",
              "\n",
              "### **Butâ€¦ Itâ€™s Still Learning!**\n",
              "Right now, quantum computers are like baby geniusesâ€”theyâ€™re **super powerful** but still make mistakes. Scientists are teaching them to get better!  \n",
              "\n",
              "---\n",
              "\n",
              "Would you like me to explain any part in a different way? ðŸ˜Š"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query DeepSeek model\n",
        "print(\"=\" * 50)\n",
        "print(\"DEEPSEEK-V3 RESPONSE:\")\n",
        "print(\"=\" * 50)\n",
        "response_deepseek = query_deepseek(prompt)\n",
        "display(Markdown(response_deepseek))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD2_UpD_l5CA"
      },
      "source": [
        "### 2. Report Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r88x6sQWCheK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
            "\n",
            "Report:\n",
            "```\n",
            "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
            "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
            "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
            "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
            "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
            "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = \"\"\"\n",
        "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
        "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
        "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
        "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
        "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
        "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
        "\n",
        "Report:\n",
        "```{report}```\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6Pi9uzJ8dObA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Here is a summary of the report on Generative AI in 5 lines:\n",
              "\n",
              "Generative AI is a technology that produces various types of content, including text, imagery, and audio. It has become more accessible with new user interfaces and advancements in machine learning algorithms. Generative AI has opened up opportunities in areas like movie dubbing and education, but also raises concerns about deepfakes and cybersecurity attacks. Recent breakthroughs in transformers and large language models have enabled generative AI to create engaging text, photorealistic images, and even videos. This technology has the potential to fundamentally change how businesses operate and could be used for tasks like code writing, product development, and supply chain transformation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query Llama model\n",
        "print(\"=\" * 50)\n",
        "print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
        "print(\"=\" * 50)\n",
        "response_llama = query_llama(prompt)\n",
        "display(Markdown(response_llama))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e9QqGVIQdTMD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DEEPSEEK-V3 RESPONSE:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "1. **Generative AI** creates content like text, images, and audio, gaining traction due to user-friendly tools.  \n",
              "2. Evolved since the 1960s, it advanced with **GANs (2014)** for realistic media, raising concerns like deepfakes.  \n",
              "3. **Transformers and large language models (LLMs)** enabled deeper analysis, multitasking across text, code, and more.  \n",
              "4. Innovations like **multimodal AI** power tools (e.g., Dall-E) for cross-media generation but face accuracy and bias issues.  \n",
              "5. Despite challenges, generative AI promises transformative impacts on industries, from coding to supply chains."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query DeepSeek model\n",
        "print(\"=\" * 50)\n",
        "print(\"DEEPSEEK-V3 RESPONSE:\")\n",
        "print(\"=\" * 50)\n",
        "response_deepseek = query_deepseek(prompt)\n",
        "display(Markdown(response_deepseek))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJTyaa37lvz9"
      },
      "source": [
        "### 3. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4ZKsd_3HaOw_"
      },
      "outputs": [],
      "source": [
        "review = \"\"\"I recently worked with this real estate company to purchase my first home,\n",
        "    and the experience was outstanding. The agent was knowledgeable, patient, and incredibly responsive.\n",
        "    They guided me through every step of the process, making what could have been a stressful\n",
        "    experience very smooth and enjoyable.\n",
        "    \"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Act as a customer review analyst, given the following customer review text,\n",
        "do the following tasks:\n",
        "- Find the sentiment (positive, negative or neutral)\n",
        "- Extract max 5 key topics or phrases of the good or bad in the review\n",
        "Review Text:\n",
        "{review}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zA6_Ayr7aR3R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Sentiment Analysis:**\n",
              "The sentiment of this review is **Positive**. The customer expresses their satisfaction with the real estate company and their agent, using words such as \"outstanding\", \"knowledgeable\", \"patient\", and \"incredibly responsive\" to describe their experience.\n",
              "\n",
              "**Key Topics or Phrases:**\n",
              "\n",
              "1. **Knowledgeable agent**: The customer praises the agent's knowledge, indicating that they were well-informed and able to provide valuable guidance throughout the process.\n",
              "2. **Patient and responsive**: The customer appreciates the agent's patience and responsiveness, suggesting that they were easy to communicate with and willing to address any concerns or questions.\n",
              "3. **Smooth and enjoyable experience**: The customer notes that the agent made the home-buying process \"smooth and enjoyable\", implying that they were able to mitigate any stress or anxiety associated with the experience.\n",
              "4. **Guided through every step**: The customer mentions that the agent guided them through every step of the process, indicating that they were proactive and supportive throughout the transaction.\n",
              "5. **Outstanding experience**: The customer describes their overall experience as \"outstanding\", emphasizing their high level of satisfaction with the real estate company and their agent."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query Llama model\n",
        "print(\"=\" * 50)\n",
        "print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
        "print(\"=\" * 50)\n",
        "response_llama = query_llama(prompt)\n",
        "display(Markdown(response_llama))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t0ID9ysnFKEz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DEEPSEEK-V3 RESPONSE:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Sentiment Analysis:  \n",
              "**Positive** â€“ The review expresses strong satisfaction with the real estate company, highlighting an \"outstanding\" experience and praising the agent's qualities.  \n",
              "\n",
              "### Key Topics/Phrases (Good Aspects):  \n",
              "1. **\"Outstanding experience\"** â€“ Overall high satisfaction with the service.  \n",
              "2. **\"Knowledgeable, patient, and incredibly responsive agent\"** â€“ Positive traits of the agent.  \n",
              "3. **\"Guided me through every step\"** â€“ Emphasis on thorough support.  \n",
              "4. **\"Made the process smooth and enjoyable\"** â€“ Stress-free and positive outcome.  \n",
              "5. **\"First home purchase\"** â€“ Context of a milestone transaction handled well.  \n",
              "\n",
              "No negative aspects were mentioned in the review."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query DeepSeek model\n",
        "print(\"=\" * 50)\n",
        "print(\"DEEPSEEK-V3 RESPONSE:\")\n",
        "print(\"=\" * 50)\n",
        "response_deepseek = query_deepseek(prompt)\n",
        "display(Markdown(response_deepseek))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Comparing Both Models Side-by-Side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PROMPT:\n",
            "======================================================================\n",
            "Write a haiku about artificial intelligence.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Metal minds awake\n",
              "Learning, growing, thinking fast\n",
              "Future's digital"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "DEEPSEEK-V3 RESPONSE:\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Silent circuits hum,**  \n",
              "**learning fast like morning lightâ€”**  \n",
              "**mind without a mind.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def compare_models(prompt, system_message=None):\n",
        "    \"\"\"\n",
        "    Compare responses from both models side by side\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"PROMPT:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(prompt)\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
        "    print(\"=\" * 70)\n",
        "    response_llama = query_llama(prompt, system_message)\n",
        "    display(Markdown(response_llama))\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"DEEPSEEK-V3 RESPONSE:\")\n",
        "    print(\"=\" * 70)\n",
        "    response_deepseek = query_deepseek(prompt, system_message)\n",
        "    display(Markdown(response_deepseek))\n",
        "\n",
        "# Example usage\n",
        "test_prompt = \"Write a haiku about artificial intelligence.\"\n",
        "compare_models(test_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
