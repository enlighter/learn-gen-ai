{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUwrTGNs4kSN"
   },
   "source": [
    "<center> <h1> Prompt Engineering with Open-Source Large Language Models (LLMs) using HuggingFace InferenceClient</h1> </center>\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGwBgKICV_DJ"
   },
   "source": [
    "\n",
    "In this notebook we will learn how to run any open-source LLMs via HuggingFace InferenceClient using this colab notebook. You can run this notebook in your local server also without worrying about having enough infrastructure to run these models!\n",
    "\n",
    "The HuggingFace [__InferenceClient__](https://huggingface.co/docs/huggingface_hub/guides/inference) provides easy access to thousands of models without worrying about your infrastructure.\n",
    "\n",
    "The models we will be trying here include:\n",
    "\n",
    "- __[meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)__ - Meta's powerful 8B parameter instruction-tuned model from the Llama 3.1 family, designed for conversational AI and instruction-following tasks.\n",
    "\n",
    "- __[deepseek-ai/DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)__ - DeepSeek's advanced V3 model optimized for various language understanding and generation tasks.\n",
    "\n",
    "\n",
    "__You just need an internet connection and a HuggingFace Account and API Key to use these models.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6K23EM5EMol"
   },
   "outputs": [],
   "source": [
    "# Install required package\n",
    "!pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uHHQuoUbEMom"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w81pr0vzhNaJ"
   },
   "source": [
    "## Get your API Key\n",
    "\n",
    "Remember to go to your [HuggingFace Account Settings](https://huggingface.co/settings/account) and generate an API key by creating a new token from the [Access Tokens](https://huggingface.co/settings/tokens) section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGSuxLmAiCGP"
   },
   "source": [
    "## Load HuggingFace API Credentials\n",
    "\n",
    "Enter your key from [here](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_YjkI6sVG7H",
    "outputId": "2ed2cb8e-20af-4d53-b13a-67e1b4e3d3f5"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter HuggingFace API Key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "API_KEY = getpass(\"Enter HuggingFace API Key: \")\n",
    "os.environ['HF_TOKEN'] = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgZzsDCZiN3k"
   },
   "source": [
    "### Initialize InferenceClient\n",
    "\n",
    "Here we create InferenceClient instances for our models. The InferenceClient provides a simple interface to access HuggingFace models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-TRno7hBB_qX"
   },
   "outputs": [],
   "source": [
    "# Initialize client with API key for models requiring authentication\n",
    "client_with_auth = InferenceClient(api_key=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# Initialize client without API key for publicly available models\n",
    "client = InferenceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BPGezswkYNZ"
   },
   "source": [
    "## Define Query Functions\n",
    "\n",
    "Here we create helper functions to query our models easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WtpZr9DlXHPZ"
   },
   "outputs": [],
   "source": [
    "def query_llama(prompt, system_message=None):\n",
    "    \"\"\"\n",
    "    Query Llama-3.1-8B-Instruct model\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    completion = client_with_auth.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        messages=messages,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_deepseek(prompt, system_message=None):\n",
    "    \"\"\"\n",
    "    Query DeepSeek-V3-0324 model\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "        messages=messages,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "repJ_4TCllJO"
   },
   "source": [
    "## Prompting with Open-Source LLM APIs\n",
    "\n",
    "Now we will use HuggingFace InferenceClient and try some tasks with prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjD6KmrBluQE"
   },
   "source": [
    "### 1. Basic Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JIyiS1TtYfMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you explain what is quantum computing to a 5th grader?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Can you explain what is quantum computing to a 5th grader?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w9y8uhKXOB7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Imagine you have a big box of LEGOs, and inside the box, there are lots of different colored blocks. Each block represents a piece of information, like a number or a letter.\n",
       "\n",
       "**Classical Computers**\n",
       "Now, imagine you want to use a special machine to build a really cool castle with those LEGO blocks. A classical computer is like a machine that follows a set of instructions to build the castle, one block at a time. It's like following a recipe to make a cake. You need to put the blocks together in a specific order, and the machine will do it step by step.\n",
       "\n",
       "**Quantum Computers**\n",
       "But, what if the machine could build the castle really, really fast? Like, in a flash of a second? That's kind of like a quantum computer. It's a machine that can use special powers called \"quantum\" to build the castle in a super-fast way.\n",
       "\n",
       "The thing about quantum computers is that they can try many different ways to build the castle at the same time! It's like the machine has a million different LEGO builders inside it, and each one is building a different castle. And then, the machine can look at all the castles and pick the best one.\n",
       "\n",
       "This is because quantum computers use something called \"quantum bits\" or \"qubits.\" Qubits are like special LEGO blocks that can be many different colors at the same time! It's like a block that's both red and blue and green all at once.\n",
       "\n",
       "So, when we use a quantum computer, it can try many different possibilities at the same time, and that makes it super powerful. It's like having a million LEGO builders working together to build the most amazing castle ever!\n",
       "\n",
       "But, it's not just LEGO castles. Quantum computers can help us solve really hard problems in science and mathematics, like understanding the universe and creating new medicines.\n",
       "\n",
       "That's what quantum computing is in a nutshell!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Llama model\n",
    "print(\"=\" * 50)\n",
    "print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "response_llama = query_llama(prompt)\n",
    "display(Markdown(response_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y3cJytUzYc4I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEEPSEEK-V3 RESPONSE:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Of course! Imagine you have a super special kind of computerâ€”a **quantum computer**â€”that doesnâ€™t just use regular \"bits\" (like tiny switches that are either ON or OFF). Instead, it uses **quantum bits**, or **qubits** (say: \"kyoo-bits\"), which are like magical switches that can be ON, OFF, **or both at the same time**!  \n",
       "\n",
       "### Hereâ€™s how itâ€™s different:  \n",
       "1. **Regular Computer**:  \n",
       "   - Thinks in 0s (OFF) and 1s (ON), like flipping a coin to heads or tails.  \n",
       "   - Solves problems one step at a time (like reading a book page by page).  \n",
       "\n",
       "2. **Quantum Computer**:  \n",
       "   - Uses qubits that can be 0, 1, **or a mix of both** (like a spinning coin thatâ€™s both heads *and* tails until you catch it).  \n",
       "   - Can test **many answers at once** (like reading every page in a book at the same time).  \n",
       "\n",
       "### Why is this cool?  \n",
       "Quantum computers can solve **super hard puzzles** (like cracking secret codes, helping design new medicines, or making AI smarter) much faster than regular computers. But theyâ€™re still being builtâ€”scientists are teaching them how to work without making too many mistakes!  \n",
       "\n",
       "Think of it like a superhero computer that uses **quantum magic** (really, just physics!) to do amazing things. ðŸš€âœ¨  \n",
       "\n",
       "Would you like a fun example or analogy to explain further? ðŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query DeepSeek model\n",
    "print(\"=\" * 50)\n",
    "print(\"DEEPSEEK-V3 RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "response_deepseek = query_deepseek(prompt)\n",
    "display(Markdown(response_deepseek))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD2_UpD_l5CA"
   },
   "source": [
    "### 2. Report Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r88x6sQWCheK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
      "\n",
      "Report:\n",
      "```\n",
      "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
      "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
      "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
      "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
      "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
      "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = \"\"\"\n",
    "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
    "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
    "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
    "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
    "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
    "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
    "\n",
    "Report:\n",
    "```{report}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6Pi9uzJ8dObA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the report in 5 lines, delimited by triple backticks:\n",
       "\n",
       "```Generative AI is a type of artificial intelligence technology that can produce various content types, including text, imagery, and audio. \n",
       "The technology has improved significantly with the introduction of generative adversarial networks (GANs) and breakthrough language models. \n",
       "Generative AI has opened up opportunities for better movie dubbing, educational content, and product design, but also raises concerns about deepfakes and cybersecurity attacks. \n",
       "Recent advances in transformers and large language models (LLMs) have enabled generative AI models to write engaging text, paint photorealistic images, and create entertaining content. \n",
       "The technology has the potential to fundamentally change enterprise technology and business operations, but also poses challenges related to accuracy, bias, and hallucinations.```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Llama model\n",
    "print(\"=\" * 50)\n",
    "print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "response_llama = query_llama(prompt)\n",
    "display(Markdown(response_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e9QqGVIQdTMD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEEPSEEK-V3 RESPONSE:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. Generative AI creates content like text, images, and audio, gaining traction due to user-friendly tools.  \n",
       "2. Originating in the 1960s, it advanced in 2014 with GANs, enabling realistic synthetic media.  \n",
       "3. While offering benefits like better dubbing, it raises concerns like deepfakes and cyber threats.  \n",
       "4. Key breakthroughsâ€”transformers and large language models (LLMs)â€”enhanced scalability and multimodal content generation.  \n",
       "5. Despite early challenges (bias, hallucinations), generative AI has transformative potential for industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query DeepSeek model\n",
    "print(\"=\" * 50)\n",
    "print(\"DEEPSEEK-V3 RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "response_deepseek = query_deepseek(prompt)\n",
    "display(Markdown(response_deepseek))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJTyaa37lvz9"
   },
   "source": [
    "### 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4ZKsd_3HaOw_"
   },
   "outputs": [],
   "source": [
    "review = \"\"\"I recently worked with this real estate company to purchase my first home,\n",
    "    and the experience was outstanding. The agent was knowledgeable, patient, and incredibly responsive.\n",
    "    They guided me through every step of the process, making what could have been a stressful\n",
    "    experience very smooth and enjoyable.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Act as a customer review analyst, given the following customer review text,\n",
    "do the following tasks:\n",
    "- Find the sentiment (positive, negative or neutral)\n",
    "- Extract max 5 key topics or phrases of the good or bad in the review\n",
    "Review Text:\n",
    "{review}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zA6_Ayr7aR3R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentiment Analysis:**\n",
       "The sentiment of this customer review is **Positive**. The customer expresses their outstanding experience with the real estate company, highlighting the agent's knowledge, patience, and responsiveness. The review also mentions that the agent made the process smooth and enjoyable, indicating a positive outcome.\n",
       "\n",
       "**Key Topics or Phrases:**\n",
       "\n",
       "1. **Knowledgeable agent**: The customer praises the agent's knowledge, suggesting that they are well-informed and capable of handling the purchase process.\n",
       "2. **Patient and responsive**: The customer appreciates the agent's patience and responsiveness, indicating that they were easy to communicate with and willing to address their concerns.\n",
       "3. **Guided through every step**: The customer mentions that the agent guided them through every step of the process, suggesting a high level of support and expertise.\n",
       "4. **Stress-free experience**: The customer states that the experience was \"smooth and enjoyable,\" implying that the agent helped to minimize stress and make the process more enjoyable.\n",
       "5. **Outstanding experience**: The customer uses the phrase \"outstanding experience\" to summarize their positive experience with the real estate company and the agent."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Llama model\n",
    "print(\"=\" * 50)\n",
    "print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "response_llama = query_llama(prompt)\n",
    "display(Markdown(response_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t0ID9ysnFKEz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEEPSEEK-V3 RESPONSE:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Sentiment Analysis:  \n",
       "**Positive**  \n",
       "\n",
       "### Key Topics/Phrases (Good Aspects):  \n",
       "1. **Outstanding experience**  \n",
       "2. **Knowledgeable agent**  \n",
       "3. **Patient and responsive service**  \n",
       "4. **Guided through every step**  \n",
       "5. **Stress-free and enjoyable process**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query DeepSeek model\n",
    "print(\"=\" * 50)\n",
    "print(\"DEEPSEEK-V3 RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "response_deepseek = query_deepseek(prompt)\n",
    "display(Markdown(response_deepseek))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Comparing Both Models Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROMPT:\n",
      "======================================================================\n",
      "Write a haiku about artificial intelligence.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LLAMA-3.1-8B-INSTRUCT RESPONSE:\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Metal minds awaken\n",
       "Intelligence in each code\n",
       "Future's digital"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "DEEPSEEK-V3 RESPONSE:\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Silent circuits humâ€”**  \n",
       "**learning, thinking, growing fast,**  \n",
       "**mind without a soul.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compare_models(prompt, system_message=None):\n",
    "    \"\"\"\n",
    "    Compare responses from both models side by side\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROMPT:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(prompt)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"LLAMA-3.1-8B-INSTRUCT RESPONSE:\")\n",
    "    print(\"=\" * 70)\n",
    "    response_llama = query_llama(prompt, system_message)\n",
    "    display(Markdown(response_llama))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DEEPSEEK-V3 RESPONSE:\")\n",
    "    print(\"=\" * 70)\n",
    "    response_deepseek = query_deepseek(prompt, system_message)\n",
    "    display(Markdown(response_deepseek))\n",
    "\n",
    "# Example usage\n",
    "test_prompt = \"Write a haiku about artificial intelligence.\"\n",
    "compare_models(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
